#' Fit and Evaluate model
#'
#' @param train_set
#' @param test_set
#' @param inducer; if input argument is an svm object then the function use it 
#'                 to evaluate the datasets. Else first fit the model and then 
#'                 evaluate.
#' @param verbose
#' 
#' @return criteria list
#' 
fit_and_evaluate_model = function(train_set,
                                  test_set,
                                  inducer=c("RF","GLM","J48","SVM"),
                                  seed=1984,
                                  verbose=TRUE){
    return_list = list()
    
    
    ###############################
    # Standardize variables names #
    ###############################
    colnames(train_set) = c(paste0("X",1:(ncol(train_set)-1)),"label")  
    colnames(test_set)  = c(paste0("X",1:(ncol(test_set)-1)),"label")  
    
    
    #################################
    # Fit model to the training-set #
    #################################
    set.seed(seed)
    if(any(class(inducer) == "svm"))
        model = inducer
    else
        model = e1071::svm(label ~ ., data=train_set, scale=T, cost=1e0, probability=TRUE)
    return_list[["Model"]] = model
    
    
    #######################################
    # Extract elements of the SVM object  #
    #######################################
    # Support Vectors information
    SV_index = rownames(model$SV)
    return_list[["SV_total"]]    = length(SV_index)
    return_list[["SV_minority"]] = table(train_set[SV_index,"label"])[["minority"]]
    return_list[["SV_majority"]] = table(train_set[SV_index,"label"])[["majority"]]
    
    
    #########################################
    # Predict the training-set and test-set #
    #########################################
    est_te = predict(model, test_set, probability=TRUE)
    est_te = attr(est_te, "probabilities")
    predictions_te = as.vector(est_te[,"minority"])
    test_set_predictions = data.frame("Index"=as.numeric(names(est_te[,"minority"])),
                                      "Minority_Probability"=as.vector(est_te[,"minority"]),
                                      "Ground_Truth"=test_set$label)
    rownames(test_set_predictions) = NULL
    
    est_tr = predict(model, train_set, probability=TRUE)
    est_tr = attr(est_tr, "probabilities")
    predictions_tr = as.vector(est_tr[,"minority"])
    train_set_predictions = data.frame("Index"=as.numeric(names(est_tr[,"minority"])),
                                       "Minority_Probability"=as.vector(est_tr[,"minority"]),
                                       "Ground_Truth"=train_set$label)
    rownames(train_set_predictions) = NULL
    
    
    ##################################
    # Evaluate model on the test-set #
    ##################################
    pred  = ROCR::prediction(predictions_te, test_set[,"label"])
    ## Precision and Recall
    PR_obj        = ROCR::performance(pred, "prec", "rec")
    Recall_vec    = unlist(PR_obj@x.values)[-50:-1] # Remove first 50 values = 0
    Precision_vec = unlist(PR_obj@y.values)[-50:-1] # Remove first 50 values = 0
    ## PRBEP (Precision Recall Break-Even Point)
    Precision_Recall_diff  = abs(Recall_vec-Precision_vec)
    PREBP_index            = which.min(Precision_Recall_diff)[1]
    return_list[["PRBEP"]] = Recall_vec[PREBP_index] #= Precision_vec[PREBP_index]
    ## AUC
    AUC_obj = ROCR::performance(pred,"auc")
    return_list[["AUC"]] = AUC_obj@y.values[[1]]
    ## Lift value as a function of "rate of poisitve predictions"
    LIFT_obj = ROCR::performance(pred, measure="lift", x.measure="rpp")
    return_list[["LIFT"]] = unlist(LIFT_obj@y.values)[1086] # unlist(LIFT_obj@x.values)[[1086]]=0.1
    
    
    ############################
    # Output model estimations #
    ############################
    return_list[["Test_set_predictions"]]  = test_set_predictions[order(test_set_predictions$Index),]
    return_list[["Train_set_predictions"]] = train_set_predictions[order(train_set_predictions$Index),]
    
    
    return(return_list)
} # end fit_and_evaluate_model