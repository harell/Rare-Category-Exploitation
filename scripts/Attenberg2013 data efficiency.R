################################################################################
# Attenberg2013 - Data efficiency
################################################################################
# options(error=recover) # debugging mode


##################
# Initialization #
##################
rm(list = ls()); cat("\014")
source("scripts/load_libraries.R")
invisible(sapply(list.files(pattern="[.]R$", path="./functions/", full.names=TRUE), source))


################
# Load dataset #
################
load_dataset(NA)
dataset_name = c("Letter",           # 1
                 "Satimage",         # 2
                 "Abalone",          # 3
                 "Adult")[+3]        # 4
DS = load_dataset(dataset_name)
p = ncol(DS)-1
n = nrow(DS)
Imb.R = table(DS[,p+1])[["majority"]]/table(DS[,p+1])[["minority"]]


##############################
# Control simulation nuances #
##############################
verbose = TRUE
store_probabilities = TRUE # Takes twich as the time
param = expand.grid(
    # Number of runs (enter series of values, such as 1:10)
    repetitions=1:20,
    # Numer of epochs (if NA then run until pool is exhausted)
    iterations=NA,
    # Unlabeled instances selection approach
    query_method=c("Random","SVM")[c(1,2)], 
    # Additional parameters to CAL
    num_query=100, 
    # Train set ranom seed 
    seed_train=2016,
    # Train set percentage (fraction) or number (integer)
    train_pct=100,
    # Tess set ranom seed
    seed_test=20160324,
    # Test set percentage (fraction) or number (integer) 
    test_pct=1/3,
    # Data imbalance ratio (if NA the original ratio is preserved)
    imbalance_ratio=NA,
    stringsAsFactors=FALSE)
param$seed_train = param$repetitions + param$seed_train
param$seed_test  = param$repetitions + param$seed_test


##############
# Simulation #
##############
start.time = Sys.time()
cl <- makeCluster(detectCores(), outfile="")   
registerDoParallel(cl)
reports_list <- foreach(
    s = 1:nrow(param),
    #.export=c("predict_set","setVariablesNames"),
    #.packages="foreach"
    #.combine=rbind,
    .options.multicore=list(preschedule=TRUE),
    .errorhandling='stop') %dopar% {#'remove',#dopar
        #####################
        # Split the dataset #
        #####################
        index_list = split_dataset(X=DS[,1:p], y=DS[,p+1],
                                   train_pct=param[s,"train_pct"], 
                                   seed_train=param[s,"seed_train"],
                                   test_pct=param[s,"test_pct"], 
                                   seed_test=param[s,"seed_test"],
                                   imbalance_ratio=param[s,"imbalance_ratio"])
        N_unlabeled = length(index_list[["index_unlabeled"]])
        N_train = length(index_list[["index_train"]])
        
        
        ##########################
        # Repetition Allocations #
        ########################## 
        r = param[s,"repetitions"]
        if(verbose) cat('\n',rep('#',40),
                        '\n','# Starting simulation ',s,'/',nrow(param),
                        '\n',rep('#',40),
                        sep='')
        report_data     = data.frame()
        if(store_probabilities){
            report_metadata = data.frame(Imb.R="GT",
                                         QueryMethod="GT",
                                         Repetition="GT",
                                         Iteration="GT",
                                         matrix(ifelse(DS[,p+1]=="majority",-1,1),nrow=1)) 
        }
        iterations = param[s,"iterations"]
        #' If iterations=NA or if number of desired iteration exceeds the number 
        #' of available iteration then run simulation until unlabeled pool is 
        #' exhausted
        Q = param[s,"num_query"]
        max_iterations = ceiling(N_unlabeled / Q)
        if(is.na(iterations) | max_iterations<iterations)
            iterations <- max_iterations
        
        
        for(i in 0:iterations){ 
            if(verbose) cat('\n','# Iteration ', i, '/', iterations, sep='')
            #######################################
            # Assign data-points into their group #
            #######################################
            DS_train     = DS[index_list[["index_train"]],]
            DS_test      = DS[index_list[["index_test"]],]
            DS_unlabeled = DS[index_list[["index_unlabeled"]],]
            DS_labeled   = DS[index_list[["index_labeled"]],]
            
            
            ##########################################
            #  Fit model on the labeled data-points  #
            #                   &                    #
            # Evaluate the model on the training set #
            ##########################################
            if(verbose) cat('\n','# Fitting and Evaluating model', sep='')
            set.seed(20160322)
            return_list = fit_and_evaluate_model(train_set=DS_labeled,
                                                 test_set=DS_test,
                                                 inducer="SVM")
            
            
            #####################
            # Store the results #
            #####################
            # 1. Data
            new_data_entry = data.frame(
                ## Simulation nuances
                Imb.R = ifelse(is.na(param[s,"imbalance_ratio"]), # If Imb.R wasn't defined then 
                               round(Imb.R,1),                    # assign the real ratio, else
                               param[s,"imbalance_ratio"]),       # assign the defined ratio
                QueryMethod=tolower(param[s,"query_method"]),
                Repetition=r,
                Iteration=i,
                ## Observations info
                ### Unlabeled set 
                Nu=nrow(DS_unlabeled),
                Nu_majority=table(DS_unlabeled$label)[["majority"]],
                Nu_minority=table(DS_unlabeled$label)[["minority"]],
                ### Labeled set
                Nl=nrow(DS_labeled),
                Nl_majority=table(DS_labeled$label)[["majority"]],
                Nl_minority=table(DS_labeled$label)[["minority"]],
                ## Evaluation criteria
                AUC=return_list$AUC,
                PRBEP=return_list$PRBEP,
                LIFT=return_list$LIFT,
                ## Support Vector info
                SV_total    = return_list$SV_total,
                SV_minority = return_list$SV_minority, 
                SV_majority = return_list$SV_majority)
            ## Run time summery statistics
            new_data_entry$Data.Efficiency = round(100*nrow(DS_labeled)/(N_unlabeled+N_train),2)
            new_data_entry$Imb.R.Empirical = round(with(new_data_entry,Nl_majority/Nl_minority),1)
            report_data = rbind(report_data,new_data_entry)
            # 2. Metadata
            if(store_probabilities){
                ## Get probabilities for all the data set
                return_list = fit_and_evaluate_model(train_set=DS_labeled,
                                                     test_set=DS,
                                                     inducer="SVM")
                est = return_list$Test_set_predictions
                prob_vector = matrix(NA, ncol=nrow(DS))
                prob_vector[est$Index] = est$Minority_Probability
                ## Subset only the estimated probabilities for the unlabeled set
                prob_vector[setdiff(est$Index,rownames(DS_unlabeled))] = NA
                new_metadata_entry = data.frame(Imb.R=as.character(new_data_entry$Imb.R),
                                                QueryMethod=as.character(new_data_entry$QueryMethod),
                                                Repetition=as.character(new_data_entry$Repetition),
                                                Iteration=as.character(new_data_entry$Iteration),
                                                prob_vector)
                report_metadata = rbind(report_metadata,new_metadata_entry)
            } # store probabilities
            
            if(verbose) cat('\n','# AUC=', round(return_list$AUC,3), sep='')
            
            
            #####################################
            # Query the next data-points to add #
            #####################################
            if(iterations==i){
                if(verbose) cat('\n','# Done', sep='')
                next
            }
            if(verbose) cat('\n','# Querying the next batch', sep='')
            
            ## Choose instances from the unlabeled pool 
            set.seed(20160322)
            if(tolower(param[s,"query_method"])=='random'){
                query_list = query_random(labeled_set=DS_labeled,
                                          unlabeled_set=DS_unlabeled,
                                          num_query=param[s,"num_query"])
            } else if(tolower(param[s,"query_method"])=='svm'){
                query_list = query_svm(labeled_set=DS_labeled,
                                       unlabeled_set=DS_unlabeled,
                                       num_query=param[s,"num_query"])  
            } # if query method
            
            ## Update index_list
            index_chosen = query_list$index
            index_list[["index_labeled"]] = union(
                index_list[["index_labeled"]],
                index_list[["index_unlabeled"]][index_chosen])
            index_list[["index_unlabeled"]] = setdiff(
                index_list[["index_unlabeled"]],
                index_list[["index_unlabeled"]][index_chosen])
            ## Sort index_list
            index_list[["index_labeled"]]   = sort(index_list[["index_labeled"]])
            index_list[["index_unlabeled"]] = sort(index_list[["index_unlabeled"]])
            
            
            if(verbose) cat('\n', '#', sep='')
        } # end iterations
        if(store_probabilities)
            return(list(report_data=report_data,report_metadata=report_metadata))
        else
            return(list(report_data=report_data))
    } # end foreach Simulation
stopCluster(cl) 
end.time = Sys.time()

###########################
# Post processing results #
###########################
# Convert lists to data.frames and join them to a single data.frame
report_data     = data.frame()
report_metadata = data.frame()

for(l in 1:length(reports_list)){
    # Data
    report_data = rbind(report_data, reports_list[[l]]$report_data)
    # Metadata
    report_metadata = rbind(report_metadata, reports_list[[l]]$report_metadata)
}
report_data = unique(report_data)
report_data = dplyr::arrange(report_data, Imb.R, QueryMethod, Repetition, Iteration)
report_metadata = unique(report_metadata)
report_metadata = dplyr::arrange(report_metadata, Imb.R, QueryMethod, Repetition, Iteration)
report_metadata[,c("Imb.R","QueryMethod","Repetition","Iteration","X1","X2","X3","X4","X5")]


#################
# Store results #
#################
Time.Diff = round(as.numeric(end.time-start.time, units = "mins"),0)
Imb.R     = paste(round(unique(param[,"imbalance_ratio"]),4), collapse = ",")
Rep.N     = length(unique(report_data$Repetition))
Epochs.N  = max(report_data$Iteration)
file_name = paste0('(',toupper(dataset_name),')',
                   '(',"Imb.R.=",Imb.R,')',
                   '(',"Reps=",Rep.N,')',
                   '(',"Epochs=",Epochs.N,')',
                   '(',Sys.Date(),')',
                   '(',paste0(Time.Diff,' minutes'),')')
# report_data
dir_path = file.path(getwd(),"results")
dir.create(dir_path, show=FALSE, recursive=TRUE)
write.csv(report_data, file.path(dir_path, paste0(file_name,".csv")), row.names=F)
# report_metadata
if(store_probabilities){
    dir_path = file.path(getwd(),"results","metadata")
    dir.create(dir_path, show=FALSE, recursive=TRUE)
    write.csv(report_metadata, file.path(dir_path, paste0(file_name,".csv")), row.names=F)
}



