################################################################################
# Attenberg2013 - Data efficiency
################################################################################
# options(error=recover) # debugging mode


##################
# Initialization #
##################
rm(list = ls()); cat("\014")
source("scripts/load_libraries.R")
invisible(sapply(list.files(pattern="[.]R$", path="./functions/", full.names=TRUE), source))


################
# Load dataset #
################
load_dataset(NA) # display data sets attributes
dataset_name = c("Letter",           # 1
                 "Satimage",         # 2
                 "Abalone",          # 3
                 "Adult")[+2]        # 4
DS = load_dataset(dataset_name)
p = ncol(DS)-1
n = nrow(DS)
Imb.R = table(DS[,p+1])[["majority"]]/table(DS[,p+1])[["minority"]]


##############################
# Control simulation nuances #
##############################
verbose = TRUE
store_probabilities = TRUE # Takes longer time
param = expand.grid(
    # Number of runs (enter series of values, such as 1:10)
    repetitions=1:16,
    # Numer of epochs (if NA then run until pool is exhausted)
    iterations=NA,
    # Explore and Exploit Strategies
    strategy=c("Max_Info","Beta","Quantile")[c(1,3)],
    # Unlabeled instances selection approach
    query_method=c("Random","SVM")[c(1,2)], 
    # Additional parameters to CAL
    num_query=100, 
    # Train set ranom seed 
    seed_train=2016,
    # Train set percentage (fraction) or number (integer)
    train_pct=100,
    # Tess set ranom seed
    seed_test=20160324,
    # Test set percentage (fraction) or number (integer) 
    test_pct=1/3,
    # Data imbalance ratio (if NA the original ratio is preserved)
    imbalance_ratio=NA,
    stringsAsFactors=FALSE)
param$seed_train = param$repetitions + param$seed_train
param$seed_test  = param$repetitions + param$seed_test
# Remove beta strategy w. query by random combinations 
param = subset(param, !(strategy %in% c("Beta","Quantile") & query_method %in% "Random"))
param = unique(param)


##############
# Simulation #
##############
start.time = Sys.time()
cl <- makeCluster(detectCores(), outfile="")   
registerDoParallel(cl)
reports_list <- foreach(
    s = 1:nrow(param),
    .options.multicore=list(preschedule=TRUE),
    .errorhandling='stop') %dopar% {#'remove',#dopar
        #####################
        # Split the dataset #
        #####################
        index_list = split_dataset(X=DS[,1:p], y=DS[,p+1],
                                   train_pct=param[s,"train_pct"], 
                                   seed_train=param[s,"seed_train"],
                                   test_pct=param[s,"test_pct"], 
                                   seed_test=param[s,"seed_test"],
                                   imbalance_ratio=param[s,"imbalance_ratio"])
        N_unlabeled = length(index_list[["index_unlabeled"]])
        N_train = length(index_list[["index_train"]])
        
        
        ##########################
        # Repetition Allocations #
        ########################## 
        Strategy    = tolower(param[s,"strategy"])
        QueryMethod = tolower(param[s,"query_method"])
        r = param[s,"repetitions"]
        if(verbose) cat('\n',rep('#',40),
                        '\n','# Starting simulation ',s,'/',nrow(param),
                        '\n',rep('#',40),
                        sep='')
        report_data = data.frame()
        if(store_probabilities){
            report_metadata = data.frame(Imb.R="GT",
                                         Strategy="GT",
                                         QueryMethod="GT",
                                         Policy="GT",
                                         Repetition="GT",
                                         Iteration="GT",
                                         matrix(ifelse(DS[,p+1]=="majority",-1,1),nrow=1)) 
        } # end storing metadata
        iterations = param[s,"iterations"]
        #' If iterations=NA or if number of desired iteration exceeds the number 
        #' of available iteration then run simulation until unlabeled pool is 
        #' exhausted
        Q = param[s,"num_query"]
        max_iterations = ceiling(N_unlabeled / Q)
        if(is.na(iterations) | max_iterations<iterations)
            iterations <- max_iterations
        
        
        for(i in 0:iterations){ 
            if(verbose) cat('\n','# Iteration ', i, '/', iterations, sep='')
            #######################################
            # Assign data-points into their group #
            #######################################
            DS_train     = DS[index_list[["index_train"]],]
            DS_test      = DS[index_list[["index_test"]],]
            DS_unlabeled = DS[index_list[["index_unlabeled"]],]
            DS_labeled   = DS[index_list[["index_labeled"]],]
            
            
            ##########################################
            #  Fit model on the labeled data-points  #
            #                   &                    #
            # Evaluate the model on the training set #
            ##########################################
            # if(verbose) cat('\n','# Fitting and Evaluating model', sep='')
            set.seed(20160322)
            return_list_te = fit_and_evaluate_model(train_set=DS_labeled,
                                                    test_set=DS_test,
                                                    inducer="SVM")
            
            
            ############################
            # Probability Measurements #
            ############################
            mdl = return_list_te[["Model"]]
            # Get probabilities for the unlabeled data set
            return_list_ul = fit_and_evaluate_model(train_set=DS_labeled,
                                                    test_set=DS_unlabeled,
                                                    inducer=mdl)
            unlabeled_est = return_list_ul$Test_set_predictions
            # Check if unlabeled data set is empty
            if(is.null(unlabeled_est$Minority_Probability)){
                unlabeled_est = data.frame(Index=0,
                                           Minority_Probability=0,
                                           Ground_Truth=NA)
            }
            # > head(unlabeled_est)
            #   Index   Minority_Probability    Ground_Truth
            #   2       0.14070540              minority
            #   4       0.12606211              majority
            #   8       0.10945538              majority
            unlabeled_est = dplyr::arrange(unlabeled_est,desc(Minority_Probability))
            # > head(unlabeled_est)
            #   Index   Minority_Probability    Ground_Truth
            #   1767    0.8201248               minority
            #   1271    0.5307919               majority
            #   3722    0.4825680               majority
            n_unlabeled_est = nrow(unlabeled_est) # \in [1,Q]
            prob_vec = unlabeled_est[["Minority_Probability"]]
            ## Fit beta distribution for the estimated probabilities of the unlabeled set
            if(length(prob_vec)>1){
                beta.fit = fitdistrplus::fitdist(prob_vec, "beta", keepdata = FALSE)
                alpha_MLE = beta.fit$estimate[1]
                beta_MLE  = beta.fit$estimate[2]   
            } else {
                alpha_MLE = 0
                beta_MLE  = 0   
            } # end Fit beta distribution if prob_vec is empty
            ## Calculate where is the (n_unlabeled_est-Q)-th order statistic
            if(length(prob_vec)>1)
                Quantile = quantile(prob_vec, probs=max(1-Q/n_unlabeled_est,0))
            else
                Quantile = 0
            ## Store
            probabilistic_entry = data.frame(
                alpha=alpha_MLE,
                beta=beta_MLE,
                Quantile=Quantile,
                Expected_Minorities=sum(head(unlabeled_est$Minority_Probability,Q)),#/n_unlabeled_est,
                Real_Minorities=sum(head(unlabeled_est$Ground_Truth,Q) %in% "minority"))#/n_unlabeled_est)
            
            
            #############################
            # Intermediate Calculations #
            #############################
            # 1. General Statistics
            general_entry = data.frame(
                ## Simulation nuances
                Imb.R = ifelse(is.na(param[s,"imbalance_ratio"]), # If Imb.R wasn't defined then 
                               round(Imb.R,1),                    # assign the real ratio, else
                               param[s,"imbalance_ratio"]),       # assign the defined ratio
                Strategy=Strategy,
                QueryMethod=QueryMethod,
                Policy=paste(Strategy,QueryMethod,sep="-"),
                Repetition=r,
                Iteration=i,
                ## Observations info
                ### Unlabeled set 
                Nu=nrow(DS_unlabeled),
                Nu_majority=table(DS_unlabeled$label)[["majority"]],
                Nu_minority=table(DS_unlabeled$label)[["minority"]],
                ### Labeled set
                Nl=nrow(DS_labeled),
                Nl_majority=table(DS_labeled$label)[["majority"]],
                Nl_minority=table(DS_labeled$label)[["minority"]],
                ## Evaluation criteria
                AUC=return_list_te$AUC,
                PRBEP=return_list_te$PRBEP,
                LIFT=return_list_te$LIFT,
                ## Support Vector info
                SV_total    = return_list_te$SV_total,
                SV_minority = return_list_te$SV_minority, 
                SV_majority = return_list_te$SV_majority,
                ## Save spot for later
                Q_Intersect = 0)
            ## Run time summery statistics
            summery_statistics_entry = data.frame(
                Data_Efficiency = round(100*nrow(DS_labeled)/(N_unlabeled+N_train),2),
                Imb.R.Empirical = round(with(general_entry,Nl_majority/Nl_minority),1))
            # 2. Metadata
            if(store_probabilities){
                ## Get probabilities for all the data set
                return_list = fit_and_evaluate_model(train_set=DS_labeled,
                                                     test_set=DS,
                                                     inducer=mdl)
                est = return_list$Test_set_predictions
                prob_vector = matrix(NA, ncol=nrow(DS))
                prob_vector[est$Index] = est$Minority_Probability
                ## Subset only the estimated probabilities for the unlabeled set
                prob_vector[setdiff(est$Index,rownames(DS_unlabeled))] = NA
                new_metadata_entry = data.frame(Imb.R=as.character(general_entry$Imb.R),
                                                Strategy=as.character(general_entry$Strategy),
                                                QueryMethod=as.character(general_entry$QueryMethod),
                                                Policy=as.character(general_entry$Policy),
                                                Repetition=as.character(general_entry$Repetition),
                                                Iteration=as.character(general_entry$Iteration),
                                                prob_vector)
                report_metadata = rbind(report_metadata, new_metadata_entry)
            } # store probabilities
            
            new_data_entry = cbind(general_entry, summery_statistics_entry, probabilistic_entry)
            report_data    = rbind(report_data, new_data_entry)
            
            if(verbose) cat('\n','# AUC=', round(return_list_te$AUC,3), sep='')
            
            
            #####################################
            # Query the next data-points to add #
            #####################################
            if(iterations==i){
                if(verbose) cat('\n','# Done', sep='')
                next
            } # end if last iteration
            
            if(verbose) cat('\n','# Querying the next batch', sep='')
            
            ## Choose instances from the unlabeled pool 
            set.seed(20160322)
            if(tolower(param[s,"query_method"])=='random'){
                query_list = query_random(labeled_set=DS_labeled,
                                          unlabeled_set=DS_unlabeled,
                                          num_query=param[s,"num_query"])
            } else if(tolower(param[s,"query_method"])=='svm'){
                query_list = query_svm(labeled_set=DS_labeled,
                                       unlabeled_set=DS_unlabeled,
                                       num_query=param[s,"num_query"])  
            } # if query method
            
            
            ################
            # Apply Policy #
            ################
            # Q instances with the highest informativeness
            unlabeled_top_Q_info_index = index_list[["index_unlabeled"]][query_list$index]
            # Q instances with the highest estimated probability of being in the minority class
            unlabeled_top_Q_prob_index = head(unlabeled_est$Index,Q)
            
            
            if(length(query_list$index)<Q){
                index_chosen = index_list[["index_unlabeled"]]
                
            } else if (Strategy=="max_info"){
                index_chosen = unlabeled_top_Q_info_index   
                
            } else if (Strategy=="beta"){
                break
                
            } else if (Strategy=="quantile"){
                epsilon   = Quantile
                union_top = union(unlabeled_top_Q_prob_index, unlabeled_top_Q_info_index)
                n1 = ceiling(epsilon*Q)
                n2 = Q-n1
                if(n1==0)       # Extreme case when epsilon = 0
                    index_chosen = unlabeled_top_Q_info_index
                else if (n2==0) # Extreme case when epsilon = 1
                    index_chosen = unlabeled_top_Q_prob_index
                else            # when epsilon \in (0,1)
                    index_chosen = c(union_top[1:n1],union_top[(n1+1):Q])
                
            }# end Policy
            
            
            # Calculate the intersection between the top Q informative observations and 
            # the Q highest minority probability
            unlabeled_top_Q_prob_info  = intersect(unlabeled_top_Q_prob_index,unlabeled_top_Q_info_index)
            Q_Intersect = length(unlabeled_top_Q_prob_info)
            report_data[nrow(report_data),"Q_Intersect"] = Q_Intersect
            
            
            ##################
            # Update Indices #
            ##################
            # Update index_list
            index_list[["index_labeled"]] = union(
                index_list[["index_labeled"]],
                index_chosen)
            index_list[["index_unlabeled"]] = setdiff(
                index_list[["index_unlabeled"]],
                index_chosen)
            # Sort index_list
            index_list[["index_labeled"]]   = sort(index_list[["index_labeled"]])
            index_list[["index_unlabeled"]] = sort(index_list[["index_unlabeled"]])
            
            
            if(verbose) cat('\n', '#', sep='')
        } # end iterations
        
        if(store_probabilities)
            return(list(report_data=report_data,report_metadata=report_metadata))
        else
            return(list(report_data=report_data))
        
    } # end foreach Simulation
stopCluster(cl) 
end.time = Sys.time()

###########################
# Post processing results #
###########################
# Convert lists to data.frames and join them to a single data.frame
report_data     = data.frame()
report_metadata = data.frame()

for(l in 1:length(reports_list)){
    # Data
    report_data = rbind(report_data, reports_list[[l]]$report_data)
    # Metadata
    report_metadata = rbind(report_metadata, reports_list[[l]]$report_metadata)
}
report_data = unique(report_data)
report_data = dplyr::arrange(report_data, Imb.R, QueryMethod, Repetition, Iteration)
report_metadata = unique(report_metadata)
report_metadata = dplyr::arrange(report_metadata, Imb.R, QueryMethod, Repetition, Iteration)
# report_metadata[,c("Imb.R","QueryMethod","Repetition","Iteration","X1","X2","X3","X4","X5")]


#################
# Store results #
#################
Time.Diff = round(as.numeric(end.time-start.time, units = "mins"),0)
Imb.R     = paste(round(unique(param[,"imbalance_ratio"]),4), collapse = ",")
Rep.N     = length(unique(report_data$Repetition))
Epochs.N  = max(report_data$Iteration)
Strategy  = paste(unique(param[,"strategy"]),collapse=",")
file_name = paste0('(',toupper(dataset_name),')',
                   '(',"Imb.R.=",Imb.R,')',
                   '(',"Strategies=",Strategy,')',
                   '(',"Reps=",Rep.N,')',
                   '(',"Epochs=",Epochs.N,')',
                   '(',Sys.Date(),')',
                   '(',paste0(Time.Diff,' minutes'),')')
# report_data
dir_path = file.path(getwd(),"results")
dir.create(dir_path, show=FALSE, recursive=TRUE)
write.csv(report_data, file.path(dir_path, paste0(file_name,".csv")), row.names=F)
# report_metadata
if(store_probabilities){
    dir_path = file.path(getwd(),"results","metadata")
    dir.create(dir_path, show=FALSE, recursive=TRUE)
    write.csv(report_metadata, file.path(dir_path, paste0(file_name,".csv")), row.names=F)
} # end if store_probabilities